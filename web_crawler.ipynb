{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Alignment\n",
    " \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel 文件 'data.xlsx' 已保存。\n"
     ]
    }
   ],
   "source": [
    "def fetch_url_content(url):\n",
    "    try:\n",
    "        # 创建 SSL 上下文，指定更灵活的配置\n",
    "        ssl_context = ssl.create_default_context()\n",
    "        ssl_context.set_ciphers('DEFAULT@SECLEVEL=1')  # 设置允许较弱的加密套件\n",
    " \n",
    "        # 发送 HTTP 请求并获取响应\n",
    "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
    "            # 读取响应内容\n",
    "            html_content = response.read().decode('utf-8')  # 假设使用 UTF-8 编码\n",
    "            return html_content\n",
    "    except urllib.error.URLError as e:\n",
    "        # 处理 URL 错误\n",
    "        print(\"URL Error:\", e)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        # 处理 HTTP 错误\n",
    "        print(\"HTTP Error:\", e)\n",
    "    except Exception as e:\n",
    "        # 处理其他异常\n",
    "        print(\"Error:\", e)\n",
    " \n",
    " \n",
    "def remove_spaces_with_replace(input_string):\n",
    "    return input_string.replace(\" \", \"\")\n",
    " \n",
    " \n",
    "def write_to_excel(data):\n",
    "    # 创建一个新的工作簿\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    " \n",
    "    # 写入表头\n",
    "    headers = [\"标题\", \"报名方式\", \"报名截止日期\", \"报名状态\", \"发布时间\"]\n",
    "    for col, header in enumerate(headers, start=1):\n",
    "        ws.cell(row=1, column=col, value=header)\n",
    "        # 设置表头单元格居中对齐\n",
    "        ws.cell(row=1, column=col).alignment = Alignment(horizontal='center', vertical='center')\n",
    " \n",
    "    # 写入数据\n",
    "    for row_idx, row_data in enumerate(data, start=2):\n",
    "        for col_idx, cell_data in enumerate(row_data, start=1):\n",
    "            ws.cell(row=row_idx, column=col_idx, value=cell_data)\n",
    " \n",
    "    # 保存 Excel 文件\n",
    "    filename = \"data.xlsx\"\n",
    "    wb.save(filename)\n",
    "    print(f\"Excel 文件 '{filename}' 已保存。\")\n",
    " \n",
    " \n",
    "data_all = []\n",
    " \n",
    " \n",
    "def earn_data(url_):\n",
    "    html_content = fetch_url_content(url_)\n",
    "    if html_content:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    " \n",
    "        # 找到类名为 idx_zbgg_hx 的表格\n",
    "        table = soup.find('table', class_='idx_zbgg_hx')\n",
    " \n",
    "        if table:\n",
    "            # 找到表格中所有行（<tr> 元素）\n",
    "            trs = table.find_all('tr')\n",
    " \n",
    "            # 遍历每个<tr>元素\n",
    "            for tr in trs:\n",
    "                # 获取<tr>元素下的所有<td>元素\n",
    "                tds = tr.find_all('td')\n",
    "                e_data = []\n",
    "                # 遍历每个<td>元素，并获取其内容\n",
    "                for td in tds:\n",
    "                    content = td.get_text().strip()\n",
    "                    # print(remove_spaces_with_replace(content))\n",
    "                    e_data.append(content)\n",
    "                if e_data:\n",
    "                    data_all.append(e_data)\n",
    "        else:\n",
    "            print(\"Table with class idx_zbgg_hx not found.\")\n",
    "        # print(data_all)\n",
    " \n",
    " \n",
    "# 测试爬虫\n",
    "if __name__ == \"__main__\":\n",
    "    num_all = ['1716623439754', '1716623814468', '1716623858932', '1716623911740', '1716623927364', '1716623943959',\n",
    "               '1716623962482', '1716623980186', '1716624002988', '1716624019847', '1716624041015', '1716624056504',\n",
    "               '1716624074168']\n",
    "    for i in range(0, len(num_all)):\n",
    "        url = 'https://bp.cfldcn.com/article!list.do?categoryCode=zbgg&request_time='+num_all[i]  # 要爬取的网页 URL\n",
    "        earn_data(url)\n",
    "    # 写入数据到 Excel 文件\n",
    "    write_to_excel(data_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
